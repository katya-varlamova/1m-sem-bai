\documentclass[12pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
%\usepackage[14pt]{extsizes}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools} 
\usepackage{pgfplots}
\usepackage{filecontents}
\usepackage{indentfirst}
\usepackage{eucal}
\usepackage{amsmath}
\usepackage{enumitem}
\frenchspacing

\usepackage{indentfirst} % Красная строка


%\usetikzlibrary{datavisualization}
%\usetikzlibrary{datavisualization.formats.functions}

\usepackage{amsmath}




% Для листинга кода:
\lstset{ %
language=haskell,                 % выбор языка для подсветки (здесь это С)
basicstyle=\small\sffamily, % размер и начертание шрифта для подсветки кода
numbers=left,               % где поставить нумерацию строк (слева\справа)
numberstyle=\tiny,           % размер шрифта для номеров строк
stepnumber=1,                   % размер шага между двумя номерами строк
numbersep=5pt,                % как далеко отстоят номера строк от подсвечиваемого кода
showspaces=false,            % показывать или нет пробелы специальными отступами
showstringspaces=false,      % показывать или нет пробелы в строках
showtabs=false,             % показывать или нет табуляцию в строках
frame=single,              % рисовать рамку вокруг кода
tabsize=2,                 % размер табуляции по умолчанию равен 2 пробелам
captionpos=t,              % позиция заголовка вверху [t] или внизу [b] 
breaklines=true,           % автоматически переносить строки (да\нет)
breakatwhitespace=false, % переносить строки только если есть пробел
escapeinside={\#*}{*)}   % если нужно добавить комментарии в коде
}

\usepackage[left=2cm,right=2cm, top=2cm,bottom=2cm,bindingoffset=0cm]{geometry}
% Для измененных титулов глав:
\usepackage{titlesec, blindtext, color} % подключаем нужные пакеты
\definecolor{gray75}{gray}{0.75} % определяем цвет
\newcommand{\hsp}{\hspace{20pt}} % длина линии в 20pt
% titleformat определяет стиль
\titleformat{\chapter}[hang]{\Huge\bfseries}{\thechapter\hsp\textcolor{gray75}{|}\hsp}{0pt}{\Huge\bfseries}


% plot
\usepackage{pgfplots}
\usepackage{filecontents}
\usetikzlibrary{datavisualization}
\usetikzlibrary{datavisualization.formats.functions}
\RequirePackage[
  style=gost-numeric,
  language=auto,
  autolang=other,
  sorting=none,
]{biblatex}

\addbibresource{bib.bib}
\begin{document}
%\def\chaptername{} % убирает "Глава"
\thispagestyle{empty}
\begin{titlepage}
	\noindent \begin{minipage}{0.15\textwidth}
	\includegraphics[width=\linewidth]{b_logo}
	\end{minipage}
	\noindent\begin{minipage}{0.9\textwidth}\centering
		\textbf{Министерство науки и высшего образования Российской Федерации}\\
		\textbf{Федеральное государственное бюджетное образовательное учреждение высшего образования}\\
		\textbf{~~~«Московский государственный технический университет имени Н.Э.~Баумана}\\
		\textbf{(национальный исследовательский университет)»}\\
		\textbf{(МГТУ им. Н.Э.~Баумана)}
	\end{minipage}
	
	\noindent\rule{18cm}{3pt}
	\newline\newline
	\noindent ФАКУЛЬТЕТ $\underline{\text{«Информатика и системы управления»}}$ \newline\newline
	\noindent КАФЕДРА $\underline{\text{«Программное обеспечение ЭВМ и информационные технологии»}}$\newline\newline\newline\newline\newline
	
	
	\begin{center}
		\noindent\begin{minipage}{1.3\textwidth}\centering
			\Large\textbf{  Отчёт по лабораторной работе №4 по дисциплине}\newline
			\textbf{ "Основы искусственного интеллекта"}\newline\newline
		\end{minipage}
	\end{center}
	
	\noindent\textbf{Тема} $\underline{\text{Основы ИНС}}$\newline\newline
	\noindent\textbf{Студент} $\underline{\text{Варламова Е. А.}}$\newline\newline
	\noindent\textbf{Группа} $\underline{\text{ИУ7-13М}}$\newline\newline
	\noindent\textbf{Оценка (баллы)} $\underline{\text{~~~~~~~~~~~~~~~~~~~~~~~~~~~}}$\newline\newline
	\noindent\textbf{Преподаватели} $\underline{\text{Строганов Ю.В.}}$\newline\newline\newline
	
	\begin{center}
		\vfill
		Москва~---~\the\year
		~г.
	\end{center}
\end{titlepage}
\large
\setcounter{page}{2}
\def\contentsname{СОДЕРЖАНИЕ}
\renewcommand{\contentsname}{СОДЕРЖАНИЕ}
\tableofcontents
\renewcommand\labelitemi{---}
\newpage
\chapter*{Введение}
\addcontentsline{toc}{chapter}{ВВЕДЕНИЕ}

Нейронные сети -- это вычислительные системы, которые способны обучаться на основе данных, распознавать сложные паттерны, и выполнять разнообразные задачи, включая классификацию, регрессию, обработку изображений, обработку естественного языка и другие задачи машинного обучения.

Целью данной лабораторной работы является создание программы, которая классифицирует данные из датасета MNIST \cite{bib:mnist} с использованием нейросетевого подхода с функциией активации ReLU и функцией потерь Cross Entropy.

Для этого необходимо решить следующие задачи:
\begin{itemize}
    \item привести постановку задачи;
    \item описать нейронную сеть: функцию активации ReLU и функцию потерь Cross Entropy;
    \item рассчитать аналитически необходимый размер обучающей выборки по неравенству Чебышёва, необходимое для гарантированного успешного выполнения поставленной задачи;
    \item привести схему нейронной сети;
    \item привести особенности реализации ПО, решающего поставленную задачу;
    \item провести исследование, состоящее в определении состояний недообучения и переобучения для различного соотношения (\% и \%) обучающей и тестовой выборок, а также для различного количества скрытых слоёв нейронной сети.
\end{itemize}

\chapter{Аналитическая часть}

\section{Постановка задачи} 
\begin{enumerate}
    \item Разработать ПО, которое классифицирует данные из датасета MNIST \cite{bib:mnist} с использованием нейросетевого подхода с функциией активации ReLU и функцией потерь Cross Entropy.

    \item Определить состояния переобучения и недообучения для

    \begin{enumerate}
        \item различного соотношения (\% и \%) обучающей и тестовой выборок:
        \begin{itemize}
            \item 10 и 90;
            \item 20 и 80;
            \item 30 и 70;
            \item 40 и 60;
            \item 50 и 50;
            \item 60 и 40;
            \item 70 и 30;
            \item 80 и 20;
            \item 90 и 10;
        \end{itemize}
        \item различного количества скрытых слоёв нейронной сети:
        \begin{itemize}
            \item 0;
            \item 1;
            \item 5;
        \end{itemize}
    \end{enumerate} 

    \item  рассчитать аналитически необходимый размер обучающей выборки по неравенству Чебышёва, необходимое для гарантированного успешного выполнения поставленной задачи.
\end{enumerate} 

\section{Описание нейронной сети}
Нейронные сети -- это вычислительные системы, которые способны обучаться на основе данных, распознавать сложные паттерны, и выполнять разнообразные задачи, включая классификацию, регрессию, обработку изображений, обработку естественного языка и другие задачи машинного обучения.
Нейронные сети состоят из множества связанных между собой элементов, называемых нейронами, которые имитируют функционирование нейронов в головном мозге.

\subsection{Основные понятия нейронной сети}
Перечислим основные понятия, исползуемые в нейросетях.
\begin{itemize}
    \item Нейрон (или узел): Нейрон -- это базовый элемент нейронной сети, имитирующий функцию биологического нейрона. 
    Он принимает входные сигналы, обрабатывает их и генерирует выходной сигнал. 
    Каждый нейрон обычно имеет несколько входов, каждый из которых соответствует весу (степени важности) исходного сигнала.
    \item Веса: Веса представляют собой параметры, которые определяют степень важности входной информации для каждого нейрона.
    Они отражают силу связей между нейронами и являются ключевыми для эффективного обучения нейронной сети.
    \item Функция активации: Функция активации определяет выходное значение нейрона на основе его взвешенного входа и возможно добавления смещения. 
    Различные функции активации, такие как ReLU, сигмоида, tanh, используются для введения нелинейности в нейронные сети.
    \item Структура и связи:  Нейронные сети могут быть организованы в различные архитектуры, такие как многослойные перцептроны, сверточные нейронные сети, рекуррентные нейронные сети и другие. 
    Связи между нейронами формируют слои и определяют поток информации в сети.
    \item Функция потерь: Функция потерь измеряет разницу между предсказанными значениями модели и фактическими значениями. 
    Во время обучения нейронная сеть минимизирует эту функцию, чтобы улучшить качество своих прогнозов.
    \item Оптимизатор: Оптимизатор используется для коррекции весов нейронов в ходе обучения, с целью минимизации функции потерь.
    Примером может служить алгоритм градиентного спуска.
    \item Слои: Нейронные сети состоят из различных слоев, таких как входной, скрытый и выходной слои. 
    Каждый слой выполняет определенные вычисления и обработку данных.
    \item Обучающие данные и цели: Нейронные сети обучаются на основе обучающих данных и их соответствующих целей (или меток). 
    Эти данные используются для корректировки весов нейронов в процессе обучения.
\end{itemize}

Для решения задачи классификации необходимо создать нейросеть. При этом должны быть решены следующие задачи:
\begin{enumerate}
    \item определена структура нейросети: определено количество слоёв, определено количество нейронов в каждом слое, заданы функции активации;
    \item проведено обучение нейросети на обучающей выборке;
    \item проведена оценка нейросети на тестовой выборке (не пересекающейся с обучающей).
\end{enumerate}

\subsection{Обучение нейросети}

Обучение нейросети состоит из нескольких эпох (итераций). В течение одной эпохи обучения нейронной сети происходит несколько этапов, которые повторяются для каждого обучающего примера:
\begin{enumerate}
    \item прямое распространение:
    \begin{itemize}
        \item входные данные подаются на входной слой нейронов;
        \item данные передаются через скрытые слои, взвешиваются с использованием соответствующих весов и агрегируются;
        \item агрегированные значения проходят через функции активации каждого нейрона в скрытых слоях и выходном слое, что приводит к формированию выходов сети;
    \end{itemize}
    \item  оценка ошибки : вычисляется ошибка между выходами сети и ожидаемыми значениями (целевыми метками/метками классов);
    \item обратное распространение ошибки:
    \begin{itemize}
        \item ошибка распространяется обратно через сеть, начиная с последнего слоя и двигаясь к входному слою;
        \item для каждого слоя вычисляется градиент функции потерь по весам и смещениям сети;
    \end{itemize}
    \item обновление весов, чтобы уменьшить ошибку модели: используя градиент ошибки, веса сети обновляются с использованием метода оптимизации, такого как стохастический градиентный спуск или его модификации;
\end{enumerate}

Эти этапы повторяются для каждой эпохи обучения с тем, чтобы постепенно корректировать веса сети и уменьшать ошибку прогноза. Процесс обучения заключается в том, чтобы минимизировать ошибку модели и достичь желаемой производительности в решении конкретной задачи.

\subsection{Функция активации ReLU}
Функция ReLU -- это популярная функция активации в нейронных сетях, которая обладает нелинейными свойствами. 
Ее формула определяется следующим образом:
\begin{equation}
    f(x) = \max(0, x) 
\end{equation}

Таким образом, ReLU функция активации принимает следующие значения:
\begin{equation}
    f(x) = \begin{cases} 
    x, & \mbox{if } x > 0 \\ 
    0, & \mbox{if } x \leq 0 
    \end{cases} 
\end{equation}

ReLU функция  подходит для использования в нейронных сетях по нескольким причинам, включая то, что она обеспечивает нелинейность (что важно для изучения сложных функций) и простоту вычисления.

\subsection{Функция потерь Cross Entropy}
Cross Entropy (кросс-энтропия) -- это функция потерь, широко применяемая в задачах оптимизации нейронных сетей, особенно в задачах классификации. 

Формула для бинарной кросс-энтропии (binary cross entropy) выглядит следующим образом:

\begin{equation}
    H(p, q) = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(p(y_i)) + (1 - y_i) \log(1 - p(y_i)) \right] 
\end{equation} 
где
\begin{itemize}
    \item \( N \) -- количество примеров в выборке
    \item \( y_i \) -- фактическое значение для i-го примера (0 или 1 в задаче бинарной классификации)
    \item \( p(y_i) \) -- предсказанная вероятность для i-го примера
\end{itemize}


Для случая многоклассовой классификации формула кросс-энтропии выглядит немного иначе:

\[ H(p, q) = -\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{M} y_{ij} \log(p_{ij}) \]
где
\begin{itemize}
    \item \( N \) -- количество примеров в выборке
    \item \( M \) -- количество классов
    \item \( y_{ij} \) -- бинарное индикаторное значение (1 или 0) для того, принадлежит ли пример \( i \) к классу \( j \)
    \item \( p_{ij} \) -- предсказанная вероятность принадлежности примера \( i \) к классу \( j \)
\end{itemize}

При обучении нейронных сетей кросс-энтропия обычно минимизируется с помощью методов оптимизации, таких как градиентный спуск или его различные вариации.


\section{Определение размера выборки аналитически}
Определим необходимый для гарантированного обучения модели размер обучающей выборки аналитически, учитывая закон больших чисел, представленный в форме второго неравенства Чебышева.

\begin{equation}
	\label{hash}
	P\left\{|X - MX| \ge \varepsilon\right\} \le \frac{\sigma^2}{N\varepsilon^2}, 
\end{equation}
где 
\begin{itemize}
    \item $N$ -- размер выборки;
    \item $X$ -- случайная величина;
    \item $\sigma^2$ -- дисперсия случайной величины;
    $MX$ -- математическое ожидание случайной величины;
    \item $\varepsilon$ -- требуемая точность.
\end{itemize}

Критерий качества выборки лежит в диапазоне от $[0;1]$. 
Пусть:
\begin{itemize}
    \item дисперсия оцениваемого показателя не превышает $0.1$;
    \item достоверность = $0.99$;
    \item точность = $0.01$.
\end{itemize}

Тогда можно выразить размер выборки, требуемый для обучения модели:

\begin{equation}
	\label{hash}
	N \ge \frac{0.1}{0.01^2 \cdot (1 - 0,99)},
\end{equation}


\begin{equation}
	\label{hash}
	N \ge 100000.
\end{equation}

Таким образом, можно заключить, что для обеспечения ошибки, не превышающей $1$\% с вероятностью $0.99$, размер обучающей выборки должен быть не менее $100000$ примеров.



\section*{Вывод}
\addcontentsline{toc}{section}{Вывод}
В данном разделе была приведена постановка задачи, описаны основные компоненты нейронных сетей, в том числе были описаны функция потерь Cross Entropy и функция активации ReLU. Кроме того, был рассчитан аналитически необходимый размер обучающей выборки по неравенству Чебышёва, необходимый для гарантированного успешного выполнения поставленной задачи.

\clearpage

\chapter{Конструкторская часть}
\section{Схема нейронной сети}
В соотвествии с заданием, необходимо построить сети, состоящие из 0, 1 и 5 скрытых слоёев. В качестве функции активациии использовать функцию ReLU. Соответствующие схемы представлены на рисунках \ref{fig:layer0_sc}, \ref{fig:layer1_sc} и \ref{fig:layer5_sc}.

Схема одного нейрона нейронной сети представлена на рисунке \ref{fig:neyron}.

\begin{figure}[h!]
  \centering
  \includegraphics[width = \linewidth]{neyron.pdf}
  \caption{Схема одного нейрона}
  \label{fig:neyron}
\end{figure}


\newpage
\begin{figure}[h!]
  \centering
  \includegraphics[scale = 0.7]{layer_0.pdf}
  \caption{Схема сети с 0 скрытых слоев}
  \label{fig:layer0_sc}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[scale = 0.8]{layer_1.pdf}
  \caption{Схема сети с 1 скрытым слоем}
  \label{fig:layer1_sc}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width = \linewidth]{layer_5.pdf}
  \caption{Схема сети с 5 скрытыми слоями}
  \label{fig:layer5_sc}
\end{figure}

\section*{Вывод}
\addcontentsline{toc}{section}{Вывод}
В данном разделе была приведена схема нейронной сети.

\chapter{Технологическая часть}

\section{Выбор средств разработки}
В качестве языка программирования был использован язык Python, поскольку этот язык кроссплатформенный и для него разработано огромное количество библиотек и модулей, решающих разнообразные задачи. 

В частности, имеются библиотеки, включающие в себя инструменты для создания нейронных сетй. В данной работе была использована библиотека <<pytorch>> \cite{bib:pytorch} для разработки нейронных сетей.

\section{Листинги ПО}

В листинге \ref{lst:nets} представлено определение структуры сетей с 0, 1 и 5 скрытыми слоями соответственно.

\begin{lstlisting}[label=lst:nets,caption=Определение структуры сетей]
class Net0(nn.Module):
    def __init__(self):
        super(Net0, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 10, bias=True)
    def forward(self, x):
        x = F.relu(self.fc1(x))
        return x
class Net1(nn.Module):
    def __init__(self):
        super(Net1, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 256, bias=True)
        self.fc2 = nn.Linear(256, 10, bias=True)
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return x
class Net5(nn.Module):
    def __init__(self, hl = 0):
        super(Net5, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 512, bias=True)
        self.fc2 = nn.Linear(512, 256, bias=True)
        self.fc3 = nn.Linear(256, 128, bias=True)
        self.fc4 = nn.Linear(128, 64, bias=True)
        self.fc5 = nn.Linear(64, 32, bias=True)
        self.fc6 = nn.Linear(32, 10, bias=True)
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = F.relu(self.fc4(x))
        x = F.relu(self.fc5(x))
        x = F.relu(self.fc6(x))
        return x
\end{lstlisting}

В листинге \ref{lst:train} представлен код обучения сетей и их оценки.

\begin{lstlisting}[label=lst:train,caption=Обучение сетей и их оценка]
def CreateNN(net, train_size=0.1, batch_size=64, epochs=40):
    ## train
    train_loader, test_loader = get_data_loaders(train_size, batch_size)
    optimizer = optim.SGD(net.parameters(), lr=1e-5, momentum=0.9)
    criterion = nn.CrossEntropyLoss()
    for epoch in range(epochs):
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = Variable(data), Variable(target)
            data = data.view(-1, 28*28)
            optimizer.zero_grad()
            net_out = net(data)
            loss = criterion(net_out, target)
            loss.backward()
            optimizer.step()
    # test
    res = []
    res.append(train_size)
    for loader in [train_loader, test_loader]:
        test_loss = 0
        correct = 0
        for data, target in loader:
            data, target = Variable(data), Variable(target)
            data = data.view(-1, 28 * 28)
            net_out = net(data)
            test_loss += criterion(net_out, target).data.item()
            pred = net_out.data.max(1)[1] 
            correct += pred.eq(target.data).sum()

        test_loss /= len(loader.dataset)
        res.append(test_loss)
        res.append(100. * correct / len(loader.dataset))
        res.append(len(loader.dataset))
    
    return res

\end{lstlisting}

\section*{Вывод}
\addcontentsline{toc}{section}{Вывод}
В данном разделе были обоснованы средства реализации ПО и реализовано ПО, решающее поставленную задачу.

\chapter{Исследовательская часть}

\section{Определение состояний недообучения}

Цель: провести исследование, состоящее в определении состояний недообучения и переобучения для различного соотношения (\% и \%) обучающей и тестовой выборок, а также для различного количества скрытых слоёв нейронной сети.

Примем, что модель обучена при точности $>70\%$. В таблицах \ref{tbl1} - \ref{tbl3} представлены результаты обучения сетей с 0, 1 и 5 скрытыми слоями с соответствующими состояниями обучения. 

Для проведения исследования использовался датасет, состоящий из 60000 картинок, который делился на тестовую и обучающую выборки. Количество эпох обучения -- 40.

\begin{table}[!h]
	\begin{center}
		\caption{\label{tbl1}Результаты обучения для 0 скрытых слоев} 
		\footnotesize
		\begin{tabular}{|l|l|l|l|l|l|l|l|}
			\hline	
   \multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Доля \\ обучающей\\ выборки\end{tabular}} & 
   \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Значение \\ функции \\ потерь на \\ обучающей \\ выборке\end{tabular}} & 
   \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Точность на\\  обучающей \\ выборке(\%)\end{tabular}} & 
    \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Размер \\  обучающей \\ выборки\end{tabular}} & 
   \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Значение \\ функции \\ потерь на\\ тестовой \\ выборке\end{tabular}} & 
   \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Точность \\на  тестовой \\ выборке \\ (\%)\end{tabular}} &
    \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Размер \\  тестовой \\ выборки\end{tabular}} &
    \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Состояние  \\ обучения \end{tabular}} \\
\hline 0.1 & 0.0247 & 57.90 & 6000 & 0.0253 & 56.54 & 54000  & недообучена \\
\hline 0.2 & 0.0132 & 80.69 & 12000 & 0.0137 & 80.03 & 48000 & обучена\\
\hline 0.3 & 0.0096 & 85.40 & 18000 & 0.0096 & 85.57 & 42000 & обучена \\
\hline 0.4 & 0.0078 & 87.38 & 24000 & 0.0080 & 87.11 & 36000 & обучена\\
\hline 0.5 & 0.0070 & 88.32 & 30000 & 0.0071 & 87.94 & 30000 & обучена \\
\hline 0.6 & 0.0065 & 88.89 & 36000 & 0.0064 & 88.77 & 24000 & обучена \\
\hline 0.7 & 0.0061 & 89.30 & 42000 & 0.0060 & 89.36 & 18000 & обучена \\
\hline 0.8 & 0.0058 & 89.60 & 48000 & 0.0055 & 90.31 & 12000 & обучена \\
\hline 0.9 & 0.0056 & 89.88 & 54000 & 0.0047 & 91.88 & 6000 & обучена \\
\hline
	\end{tabular}
	\end{center}
\end{table}

\begin{table}[!h]
	\begin{center}
		\caption{\label{tbl2}Результаты обучения для 1 скрытого слоя} 
		\footnotesize
		\begin{tabular}{|l|l|l|l|l|l|l|l|}
			\hline	
   \multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Доля \\ обучающей\\ выборки\end{tabular}} & 
   \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Значение \\ функции \\ потерь на \\ обучающей \\ выборке\end{tabular}} & 
   \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Точность на\\  обучающей \\ выборке(\%)\end{tabular}} & 
    \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Размер \\  обучающей \\ выборки\end{tabular}} & 
   \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Значение \\ функции \\ потерь на\\ тестовой \\ выборке\end{tabular}} & 
   \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Точность \\на  тестовой \\ выборке \\ (\%)\end{tabular}} &
    \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Размер \\  тестовой \\ выборки\end{tabular}} &
    \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Состояние  \\ обучения \end{tabular}} \\
\hline 0.1 & 0.0341 & 30.08 & 6000 & 0.0340 & 30.67 & 54000 & недообучена \\
\hline 0.2 & 0.0280 & 52.91 & 12000 & 0.0282 & 51.81 & 48000 & недообучена \\
\hline 0.3 & 0.0230 & 59.29 & 18000 & 0.0230 & 59.58 & 42000 & недообучена \\
\hline 0.4 & 0.0198 & 62.88 & 24000 & 0.0199 & 62.70 & 36000 & недообучена \\
\hline 0.5 & 0.0160 & 71.56 & 30000 & 0.0162 & 71.09 & 30000 & обучена\\
\hline 0.6 & 0.0142 & 72.64 & 36000 & 0.0142 & 72.38 & 24000 & обучена\\
\hline 0.7 & 0.0132 & 73.19 & 42000 & 0.0132 & 73.22 & 18000 & обучена\\
\hline 0.8 & 0.0126 & 73.61 & 48000 & 0.0124 & 73.87 & 12000 & обучена\\
\hline 0.9 & 0.0121 & 73.93 & 54000 & 0.0114 & 75.15 & 6000 & обучена\\
\hline
	\end{tabular}
	\end{center}
\end{table}

\begin{table}[!h]
	\begin{center}
		\caption{\label{tbl3}Результаты обучения для 5 скрытых слоев} 
		\footnotesize
		\begin{tabular}{|l|l|l|l|l|l|l|l|}
			\hline	
   \multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Доля \\ обучающей\\ выборки\end{tabular}} & 
   \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Значение \\ функции \\ потерь на \\ обучающей \\ выборке\end{tabular}} & 
   \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Точность на\\  обучающей \\ выборке(\%)\end{tabular}} & 
    \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Размер \\  обучающей \\ выборки\end{tabular}} & 
   \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Значение \\ функции \\ потерь на\\ тестовой \\ выборке\end{tabular}} & 
   \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Точность \\на  тестовой \\ выборке \\ (\%)\end{tabular}} &
    \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Размер \\  тестовой \\ выборки\end{tabular}} &
    \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Состояние  \\ обучения \end{tabular}} \\
\hline 0.1 & 0.0361 & 10.13 & 6000 & 0.0360 & 9.83 & 54000 & недообучена \\
\hline 0.2 & 0.0361 & 10.07 & 12000 & 0.0360 & 9.81 & 48000 & недообучена \\
\hline 0.3 & 0.0360 & 11.47 & 18000 & 0.0360 & 11.41 & 42000 & недообучена \\
\hline 0.4 & 0.0358 & 17.52 & 24000 & 0.0359 & 17.05 & 36000 & недообучена \\
\hline 0.5 & 0.0357 & 16.86 & 30000 & 0.0357 & 16.01 & 30000 & недообучена \\
\hline 0.6 & 0.0355 & 22.67 & 36000 & 0.0354 & 22.36 & 24000 & недообучена \\
\hline 0.7 & 0.0347 & 28.18 & 42000 & 0.0348 & 28.17 & 18000 & недообучена \\
\hline 0.8 & 0.0316 & 33.52 & 48000 & 0.0316 & 34.10 & 12000 & недообучена \\
\hline 0.9 & 0.0207 & 59.98 & 54000 & 0.0199 & 62.13 & 6000 & недообучена \\
\hline
	\end{tabular}
	\end{center}
\end{table}


Видим, что в большинстве случаев точность растет с увеличением размера обучающей выборки, однако для первых двух сетей (с 0 и 1 скрытыми слоями) можно заметить, что с определённой доли обучающей выборки (0.7 и 0.5 соответственно) с увеличением размера выборки точность остается почти постоянной. Это означает, что с еще большим увеличением обучающей выборки сети начнут переобучаться.

Кроме того, по результам для сети с 5 скрытыми слоями можно сделать вывод, что размер выборки слишком мал для обучения такой сети (даже при доле обучающей выборки 0.9 точность менее 60\%).  

\section*{Вывод}
\addcontentsline{toc}{section}{Вывод}
В данном разделе было проведено исследование, состоящее в определении состояний недообучения и переобучения для различного соотношения (\% и \%) обучающей и тестовой выборок, а также для различного количества скрытых слоёв нейронной сети. Были сделаны соответствующие выводы.

\chapter*{ЗАКЛЮЧЕНИЕ}
\addcontentsline{toc}{chapter}{ЗАКЛЮЧЕНИЕ}
Целью данной лабораторной работы являлось создание программы, которая классифицирует данные из датасета MNIST \cite{bib:mnist} с использованием нейросетевого подхода с функциией активации ReLU и функцией потерь Cross Entropy. 
Цель работы была достигнута.
Для этого были решены следующие задачи:
\begin{itemize}
    \item приведена постановка задачи;
    \item описана нейронная сеть: функция активации ReLU и функция потерь Cross Entropy;
    \item рассчитан аналитически необходимый размер обучающей выборки по неравенству Чебышёва, необходимый для гарантированного успешного выполнения поставленной задачи;
    \item приведена схема нейронной сети;
    \item приведены особенности реализации ПО, решающего поставленную задачу;
    \item проведено исследование, состоящее в определении состояний недообучения и переобучения для различного соотношения (\% и \%) обучающей и тестовой выборок, а также для различного количества скрытых слоёв нейронной сети.
\end{itemize}

В результате исследования было выяснено, что в большинстве случаев точность растет с увеличением размера обучающей выборки, однако для первых двух сетей (с 0 и 1 скрытым слоем) можно заметить, что с определённой доли обучающей выборки (0.7 и 0.5 соответственно) с увеличением размера выборки точность остается почти постоянной. Это означает, что с еще большим увеличением обучающей выборки сети начнут переобучаться.

Кроме того, по результам для сети с 5 скрытыми слоями можно сделать вывод, что размер выборки слишком мал для обучения такой сети (даже при доле обучающей выборки 0.9 точность менее 60\%).  
\printbibliography[title={СПИСОК ИСПОЛЬЗОВАННЫХ\\ ИСТОЧНИКОВ}]
\addcontentsline{toc}{chapter}{СПИСОК ИСПОЛЬЗОВАННЫХ ИСТОЧНИКОВ}

\end{document}